# -*- coding: utf-8 -*-
"""Sentimental Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wqsgzSl6coqf8LeqHRI1b29ZySo4FVxA

#Required Packages
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import learning_curve, validation_curve
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import log_loss, matthews_corrcoef
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D
from keras.layers import LSTM

"""#Load the Dataset"""

df = pd.read_csv("imdbdataset.csv")
print(df.head())

"""#Preprocessing of Data

##Lowercasing
"""

df['review'] = df['review'].str.lower()
print(df.head())

"""##Removing HTML Tags"""

def remove_html_tags(text):
    pattern = re.compile('<[^<]+?>')
    return pattern.sub(r'', text)

df['review'] = df['review'].apply(remove_html_tags)

print(df.head())

"""##Removing Punctuation marks"""

def remove_punctuation(text):
    return re.sub(r'[^\w\s]', '', text)

df['review'] = df['review'].apply(remove_punctuation)

print(df.head())

"""##Removing URL's"""

def remove_url(text):
    return re.sub(r'http[s]?://\S+|www\.\S+', '', text)

df['review'] = df['review'].apply(remove_url)

"""##Removing extra white spaces"""

def remove_extra_whitespaces(text):
    return re.sub(' +', ' ', text).strip()

df['review'] = df['review'].apply(remove_extra_whitespaces)

print(df.head())

"""##Removing stop words"""

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    lst = []
    for word in text.split():
        if word not in stop_words:
            lst.append(word)
    return " ".join(lst)

print(df['review'][1])

df['review'] = df['review'].apply(remove_stopwords)

print(df['review'][1])

print(df.head())

"""##Removing Numbers"""

def remove_numbers(text):
    lst = []
    for word in text.split():
      if not word.isdigit():
        lst.append(word)
    return " ".join(lst)

df['review'] = df['review'].apply(remove_numbers)

print(df.head())

"""##Word Tokenization"""

def tokenize(text):
    return word_tokenize(text)

df['review'] = df['review'].apply(tokenize)

print(df.head())

print(df.review[1])

"""##Word Lemmatization"""

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(word) for word in text]

df['review'] = df['review'].apply(lemmatize_text)

print(df.head())

print(df.review[1])

"""##Convert sentiment to Numeric values"""

df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

print(df.head())

"""#Split the dataset into Train, Test and Validation Data"""

X = df['review'].apply(lambda x: ' '.join(x))
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

print("Train set:", len(X_train))
print("Validation set:", len(X_val))
print("Test set:", len(X_test))

"""#Implementation of Machine Learning Algorithms

##Logistic Regression

###Implementation and Training
"""

param_grid = {
    'tfidf__max_features': [1000, 5000, 10000],
    'lr__C': [0.1, 1.0, 10.0]
}

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('lr', LogisticRegression())
])

grid_search_lr = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)

grid_search_lr.fit(X_train, y_train)

best_hyper_params = grid_search_lr.best_params_
best_lr_model = grid_search_lr.best_estimator_
best_lr_model.fit(X_train, y_train)

"""###Performance Metrics"""

val_accuracy_lr = best_lr_model.score(X_val, y_val)
test_accuracy_lr = best_lr_model.score(X_test, y_test)
y_pred_lr = best_lr_model.predict(X_test)
y_pred_proba_lr = best_lr_model.predict_proba(X_test)
conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)
class_report_lr = classification_report(y_test, y_pred_lr)
logloss_lr = log_loss(y_test, y_pred_proba_lr)
mcc_lr = matthews_corrcoef(y_test, y_pred_lr)

print("Best Training Parameters:", best_hyper_params)
print("Best Training Accuracy:", grid_search_lr.best_score_)
print("Validation Accuracy of LR:", val_accuracy_lr)
print("Test Accuracy of LR:", test_accuracy_lr)
print("Log Loss of LR:", logloss_lr)
print("Matthews Correlation Coefficient of LR:", mcc_lr)

print("Classification Report of LR:")
print(class_report_lr)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_lr, ['Negative', 'Neutral', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_lr[:,1], pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_lr[:,1], pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

train_sizes, train_scores, valid_scores = learning_curve(best_lr_model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.title("Learning Curve")
plt.legend(loc="best")
plt.show()

"""####Validation Curve"""

param_range = [1000, 5000, 10000, 0.1, 1.0, 10.0]
train_scores, valid_scores = validation_curve(best_lr_model, X_train, y_train, param_name="tfidf__max_features", param_range=param_range, cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(param_range, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(param_range, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Max Features")
plt.ylabel("Score")
plt.title("Validation Curve")
plt.legend(loc="best")
plt.show()

"""##Naive Bayes Classifier

###Implementation and Training
"""

param_grid = {
    'tfidf__max_features': [1000, 5000, 10000],
    'nb__alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],
    'nb__fit_prior': [True, False]
}

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('nb', MultinomialNB())
])

grid_search_nb = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')

grid_search_nb.fit(X_train, y_train)

best_hyper_params = grid_search_nb.best_params_
best_nb_model = grid_search_nb.best_estimator_
best_nb_model.fit(X_train, y_train)

"""###Performance Metrics"""

val_accuracy_nb = best_nb_model.score(X_val, y_val)
test_accuracy_nb = best_nb_model.score(X_test, y_test)
y_pred_nb = best_nb_model.predict(X_test)
y_pred_proba_nb = best_nb_model.predict_proba(X_test)
conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)
class_report_nb = classification_report(y_test, y_pred_nb)
logloss_nb = log_loss(y_test, y_pred_proba_nb)
mcc_nb = matthews_corrcoef(y_test, y_pred_nb)

print("Best Training Parameters:", best_hyper_params)
print("Best Training Accuracy:", grid_search_nb.best_score_)
print("Validation Accuracy of NaiveBayes:", val_accuracy_nb)
print("Test Accuracy of NaiveBayes:", test_accuracy_nb)
print("Log Loss of NaiveBayes:", logloss_nb)
print("Matthews Correlation Coefficient of NaiveBayes:", mcc_nb)

print("Classification Report of NaiveBayes:")
print(class_report_nb)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_nb, ['Negative', 'Neutral', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_nb[:,1], pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_nb[:,1], pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

train_sizes, train_scores, valid_scores = learning_curve(best_nb_model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.title("Learning Curve")
plt.legend(loc="best")
plt.show()

"""####Validation Curve"""

param_range = [1000, 5000, 10000, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
train_scores, valid_scores = validation_curve(best_nb_model, X_train, y_train, param_name="tfidf__max_features", param_range=param_range, cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(param_range, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(param_range, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Max Features")
plt.ylabel("Score")
plt.title("Validation Curve")
plt.legend(loc="best")
plt.show()

"""##Random Forest Classifier

###Implementation and Training
"""

param_grid = {
    'tfidf__max_features': [1000, 5000, 10000],
    'rf__n_estimators': [100, 200, 300],
    'rf__max_depth': [10, 20, 30]
}

pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('rf', RandomForestClassifier())
])

grid_search_rf = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)

import os
os.environ["JAX_PLATFORM_NAME"] = "cpu"

grid_search_rf.fit(X_train, y_train)

best_hyper_params = grid_search_rf.best_params_
best_rf_model = grid_search_rf.best_estimator_
best_rf_model.fit(X_train, y_train)

"""###Performance Metrics"""

val_accuracy_rf = best_rf_model.score(X_val, y_val)
test_accuracy_rf = best_rf_model.score(X_test, y_test)
y_pred_rf = best_rf_model.predict(X_test)
y_pred_proba_rf = best_rf_model.predict_proba(X_test)
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
class_report_rf = classification_report(y_test, y_pred_rf)
logloss_rf = log_loss(y_test, y_pred_proba_rf)
mcc_rf = matthews_corrcoef(y_test, y_pred_rf)

print("Best Training Parameters:", best_hyper_params)
print("Best Training Accuracy:", grid_search_rf.best_score_)
print("Validation Accuracy of RandomForest:", val_accuracy_rf)
print("Test Accuracy of RandomForest:", test_accuracy_rf)
print("Log Loss of RandomForest:", logloss_rf)
print("Matthews Correlation Coefficient of RandomForest:", mcc_rf)

print("Classification Report of RandomForest:")
print(class_report_rf)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_rf, ['Negative', 'Neutral', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_rf[:,1], pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_rf[:,1], pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

train_sizes, train_scores, valid_scores = learning_curve(best_rf_model, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(train_sizes, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Training examples")
plt.ylabel("Score")
plt.title("Learning Curve")
plt.legend(loc="best")
plt.show()

"""####Validation Curve"""

param_range = [1000, 5000, 10000, 100, 200, 300, 10, 20, 30]
train_scores, valid_scores = validation_curve(best_rf_model, X_train, y_train, param_name="tfidf__max_features", param_range=param_range, cv=3)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
valid_scores_mean = np.mean(valid_scores, axis=1)
valid_scores_std = np.std(valid_scores, axis=1)

plt.figure(figsize=(5, 5))
plt.plot(param_range, train_scores_mean, 'o-', color="r", label="Training score")
plt.plot(param_range, valid_scores_mean, 'o-', color="g", label="Cross-validation score")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color="g")
plt.xlabel("Max Features")
plt.ylabel("Score")
plt.title("Validation Curve")
plt.legend(loc="best")
plt.show()

"""#Creating an embedding layer for the first layer of Neural Network"""

max_words = 10000
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)
sequences = tokenizer.texts_to_sequences(X_train)

maxlen = 100
X_train_pad = pad_sequences(sequences, maxlen=maxlen)
X_val_pad = pad_sequences(tokenizer.texts_to_sequences(X_val), maxlen=maxlen)
X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=maxlen)
word_index = tokenizer.word_index
reverse_word_index = {v: k for k, v in word_index.items()}

embedding_dim = 100
embedding_matrix = np.zeros((max_words, embedding_dim))
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        if word in word_index and word_index[word] < max_words:
            embedding_matrix[word_index[word]] = np.array(values[1:], dtype='float32')

"""#Neural Network Algorithms

##Simple Neural Network

###Implementation and Training
"""

param_grid = {
    'dropout_rate': [0.2, 0.3, 0.4]
}

def create_model(embedding_matrix, maxlen, embedding_dim=100, max_words=10000, dropout_rate=0.2):
    model = Sequential()
    model.add(Embedding(embedding_matrix.shape[0],
                        embedding_matrix.shape[1],
                        input_length=maxlen,
                        weights=[embedding_matrix],
                        trainable=False))
    model.add(Flatten())
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

best_val_acc_nn = 0
best_model_nn = None
best_hyperparams = None
best_model_nn_history = None

train_scores = []
val_scores = []

for dropout_rate in param_grid['dropout_rate']:
    model = create_model(embedding_matrix=embedding_matrix, maxlen=maxlen, embedding_dim=embedding_dim, max_words=max_words, dropout_rate=dropout_rate)
    early_stopping = EarlyStopping(monitor='val_loss', patience=3)
    history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1,
              validation_data=(X_val_pad, y_val), callbacks=[early_stopping])
    val_loss, val_acc = model.evaluate(X_val_pad, y_val)
    train_acc = history.history['accuracy'][-1]
    train_scores.append(train_acc)
    val_scores.append(val_acc)
    if val_acc > best_val_acc_nn:
        best_val_acc_nn = val_acc
        best_model_nn = model
        best_model_nn_history = history
        best_hyperparams = {'dropout_rate': dropout_rate}

"""###Performance Metrics"""

test_loss_nn, test_acc_nn = best_model_nn.evaluate(X_test_pad, y_test)
y_pred_proba_nn = best_model_nn.predict(X_test_pad)
y_pred_nn = (y_pred_proba_nn > 0.5).astype("int32")
conf_matrix_nn = confusion_matrix(y_test, y_pred_nn)
class_report_nn = classification_report(y_test, y_pred_nn)
logloss_nn = log_loss(y_test, y_pred_proba_nn)
mcc_nn = matthews_corrcoef(y_test, y_pred_nn)

print("Best Hyperparameters of NN:", best_hyperparams)
print("Best Training Accuracy:", best_val_acc_nn)
print("Testing Accuracy with best model:", test_acc_nn)
print("Log Loss of Neural Network:", logloss_nn)
print("Matthews Correlation Coefficient of Neural Network:", mcc_nn)
print("Best Neural Network model Summary:")
print(best_model_nn.summary())

print("Classification Report of Neural Network:")
print(class_report_nn)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_nn, ['Negative', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_nn, pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_nn, pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

plt.plot(best_model_nn_history.history['accuracy'], label='train_accuracy')
plt.plot(best_model_nn_history.history['val_accuracy'], label='val_accuracy')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

"""####Validation Curve"""

dropout_rates = param_grid['dropout_rate']
def plot_validation_curve(dropout_rates, train_scores, val_scores):
    plt.plot(dropout_rates, train_scores, marker='o', label='Training Accuracy')
    plt.plot(dropout_rates, val_scores, marker='o', label='Validation Accuracy')
    plt.title('Validation Curve')
    plt.xlabel('Dropout Rate')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()
    plt.show()

plot_validation_curve(dropout_rates, train_scores, val_scores)

"""##Convolution Neural Network

###Implementation and Training
"""

param_grid = {
    'dropout_rate': [0.2, 0.3, 0.4]
}

train_scores = []
val_scores = []

def create_cnn_model(embedding_matrix, maxlen, embedding_dim=100, max_words=10000, dropout_rate=0.2):
    model = Sequential()
    model.add(Embedding(embedding_matrix.shape[0],
                        embedding_matrix.shape[1],
                        input_length=maxlen,
                        weights=[embedding_matrix],
                        trainable=False))
    model.add(Conv1D(32, 5, activation='relu'))
    model.add(MaxPooling1D(5))
    model.add(Conv1D(32, 5, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

best_val_acc_cnn = 0
best_model_cnn = None
best_hyperparams_cnn = None
best_model_cnn_history = None

train_scores = []
val_scores = []

for dropout_rate in param_grid['dropout_rate']:
    model = create_cnn_model(embedding_matrix=embedding_matrix, maxlen=maxlen, embedding_dim=embedding_dim, max_words=max_words, dropout_rate=dropout_rate)
    early_stopping = EarlyStopping(monitor='val_loss', patience=3)
    history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1,
              validation_data=(X_val_pad, y_val), callbacks=[early_stopping])
    val_loss, val_acc = model.evaluate(X_val_pad, y_val)
    train_acc = history.history['accuracy'][-1]
    train_scores.append(train_acc)
    val_scores.append(val_acc)
    print("Dropout Rate:", dropout_rate)
    print("Training Accuracy:", train_acc)
    print("Validation Accuracy:", val_acc)
    if val_acc > best_val_acc_cnn:
        best_val_acc_cnn = val_acc
        best_model_cnn = model
        best_model_cnn_history = history
        best_hyperparams_cnn = {'dropout_rate': dropout_rate}

"""###Performance Metrics"""

test_loss_cnn, test_acc_cnn = best_model_cnn.evaluate(X_test_pad, y_test)
y_pred_proba_cnn = best_model_cnn.predict(X_test_pad)
y_pred_cnn = (y_pred_proba_cnn > 0.5).astype("int32")
conf_matrix_cnn = confusion_matrix(y_test, y_pred_cnn)
class_report_cnn = classification_report(y_test, y_pred_cnn)
logloss_cnn = log_loss(y_test, y_pred_proba_cnn)
mcc_cnn = matthews_corrcoef(y_test, y_pred_cnn)

print("Best Hyperparameters of CNN:", best_hyperparams_cnn)
print("Best Training Accuracy:", best_val_acc_cnn)
print("Testing Accuracy with best CNN model:", test_acc_cnn)
print("Log Loss of CNN :", logloss_cnn)
print("Matthews Correlation Coefficient of CNN:", mcc_cnn)
print("Best Model CNN Summary:")
print(best_model_cnn.summary())

print("Classification Report of CNN:")
print(class_report_cnn)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_cnn, ['Negative', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_cnn, pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_cnn, pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

plt.plot(best_model_cnn_history.history['accuracy'], label='train_accuracy')
plt.plot(best_model_cnn_history.history['val_accuracy'], label='val_accuracy')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

"""####Validation Curve"""

dropout_rates = param_grid['dropout_rate']
def plot_validation_curve(dropout_rates, train_scores, val_scores):
    plt.plot(dropout_rates, train_scores, marker='o', label='Training Accuracy')
    plt.plot(dropout_rates, val_scores, marker='o', label='Validation Accuracy')
    plt.title('Validation Curve')
    plt.xlabel('Dropout Rate')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()
    plt.show()

plot_validation_curve(dropout_rates, train_scores, val_scores)

"""##LSTM

###Implementation and Training
"""

param_grid_lstm = {
    'dropout_rate': [0.2, 0.3, 0.4]
}

def create_lstm_model(embedding_matrix, maxlen, embedding_dim=100, max_words=10000, dropout_rate=0.2):
    model = Sequential()
    model.add(Embedding(embedding_matrix.shape[0],
                        embedding_matrix.shape[1],
                        input_length=maxlen,
                        weights=[embedding_matrix],
                        trainable=False))
    model.add(LSTM(32))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

best_val_acc_lstm = 0
best_model_lstm = None
best_model_lstm_history = history
best_hyperparams_lstm = None

train_scores = []
val_scores = []

for dropout_rate in param_grid['dropout_rate']:
    model = create_lstm_model(embedding_matrix=embedding_matrix, maxlen=maxlen, embedding_dim=embedding_dim, max_words=max_words, dropout_rate=dropout_rate)
    early_stopping = EarlyStopping(monitor='val_loss', patience=3)
    history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1,
              validation_data=(X_val_pad, y_val), callbacks=[early_stopping])
    val_loss, val_acc = model.evaluate(X_val_pad, y_val)
    train_acc = history.history['accuracy'][-1]
    train_scores.append(train_acc)
    val_scores.append(val_acc)
    if val_acc > best_val_acc_lstm:
        best_val_acc_lstm = val_acc
        best_model_lstm = model
        best_model_lstm_history = history
        best_hyperparams_lstm = {'dropout_rate': dropout_rate}

"""###Performance Metrics"""

test_loss_lstm, test_acc_lstm = best_model_lstm.evaluate(X_test_pad, y_test)
y_pred_proba_lstm = best_model_lstm.predict(X_test_pad)
y_pred_lstm = (y_pred_proba_lstm > 0.5).astype("int32")
conf_matrix_lstm = confusion_matrix(y_test, y_pred_lstm)
class_report_lstm = classification_report(y_test, y_pred_lstm)
logloss_lstm = log_loss(y_test, y_pred_proba_lstm)
mcc_lstm = matthews_corrcoef(y_test, y_pred_lstm)

print("Best Hyperparameters of LSTM:", best_hyperparams_lstm)
print("Best Training Accuracy:", best_val_acc_lstm)
print("Testing Accuracy with best LSTM model:", test_acc_lstm)
print("Log Loss of LSTM :", logloss_lstm)
print("Matthews Correlation Coefficient of LSTM:", mcc_lstm)
print("Best Model LSTM Summary:")
print(best_model_lstm.summary())

print("Classification Report of LSTM:")
print(class_report_lstm)

"""###Performance representations and Curves

####Confusion Matrix
"""

def plot_confusion_matrix(conf_matrix, labels):
    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

plot_confusion_matrix(conf_matrix_lstm, ['Negative', 'Positive'])

"""####Precision-Recall Curve"""

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_lstm, pos_label=1)
plt.figure(figsize=(5, 5))
plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""####ROC Curve"""

fpr, tpr, _ = roc_curve(y_test, y_pred_proba_lstm, pos_label=1)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(5, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

"""####Learning Curve"""

plt.plot(best_model_lstm_history.history['accuracy'], label='train_accuracy')
plt.plot(best_model_lstm_history.history['val_accuracy'], label='val_accuracy')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()

"""####Validation Curve"""

def plot_validation_curve(dropout_rates, train_scores, val_scores):
    plt.plot(dropout_rates, train_scores, marker='o', label='Training Accuracy')
    plt.plot(dropout_rates, val_scores, marker='o', label='Validation Accuracy')
    plt.title('Validation Curve')
    plt.xlabel('Dropout Rate')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()
    plt.show()

plot_validation_curve(dropout_rates, train_scores, val_scores)